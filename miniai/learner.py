# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_learner.ipynb.

# %% auto 0
__all__ = ['torch_device', 'device', 'Learner', 'Callback', 'run_cbs', 'CancelFitException', 'CancelBatchException',
           'CancelEpochException', 'to_cpu', 'M', 'Recorder', 'MetricsCB', 'DeviceCB', 'with_cbs', 'ProgressCB',
           'TensorboardCB', 'Config', 'WandBCB', 'MomentumLearner', 'LRFinderCB', 'lr_find', 'SingleBatchCB']

# %% ../nbs/09_learner.ipynb 1
import math
import torch
from torch import nn, tensor
import matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy
from tqdm.auto import trange, tqdm

from torch import optim
import torch.nn.functional as F

from .conv import *
from fastprogress import progress_bar, master_bar

torch_device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
device = torch.device(torch_device)

# %% ../nbs/09_learner.ipynb 13
class Learner: pass

class Callback: 
    order = 0
    
    def before_fit(self, learn: Learner): pass
    def after_fit(self, learn: Learner): pass
    def before_epoch(self, learn: Learner): pass
    def after_epoch(self, learn: Learner): pass
    def before_batch(self, learn: Learner): pass
    def after_batch(self, learn: Learner): pass

# %% ../nbs/09_learner.ipynb 14
def run_cbs(cbs, method_name, learn=None):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_name, None)
        if method is not None: method(learn)

# %% ../nbs/09_learner.ipynb 15
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/09_learner.ipynb 27
from torcheval.metrics import MulticlassAccuracy, Mean

# %% ../nbs/09_learner.ipynb 31
def to_cpu(x):
    if isinstance(x, Mapping): return {k: to_cpu(v) for k, v in x.items()}
    if isinstance(x, list): return [to_cpu(o) for o in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    return x.detach().cpu()

# %% ../nbs/09_learner.ipynb 32
class M(dict):
    def __init__(self, d, epoch, train=True): 
        super().__init__(d)
        self.train = train
        self.epoch = epoch

class Recorder(list):
    def __init__(self, *items): super().__init__(items)
    def add(self, d, epoch, train): self.append(M(d, epoch, train))
    def get_latest(self):
        latest = dict(self[-1].items())
        latest['epoch'] = self[-1].epoch
        latest['train'] = self[-1].train   
        return latest

    def log_latest(self): print(self.get_latest())
        
    
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.recorder = Recorder()
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()
        
    def before_fit(self, learn): 
        learn.metrics = self
        learn.recorder = self.recorder
        
    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]
    
    def after_epoch(self, learn):
        train = 'train' if learn.model.training else 'valid'
        self.recorder.add({k: v.compute().item() for k, v in self.all_metrics.items()}, 
                          learn.epoch, train)
        self._log(self.recorder)
    
    def _log(self, recorder): recorder.log_latest()
    
    def after_batch(self, learn):
        with torch.no_grad():
            x, y, *_ = to_cpu(learn.batch)
            for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)
            self.loss.update(to_cpu(learn.loss), weight=len(x))

# %% ../nbs/09_learner.ipynb 33
class DeviceCB(Callback):
    def __init__(self, device=device): self.device = device
    def before_fit(self, learn):
        if hasattr(learn.model, 'to'): learn.model.to(self.device)
    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)

# %% ../nbs/09_learner.ipynb 37
class with_cbs:
    def __init__(self, name): self.name = name
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o._callback(f'before_{self.name}')
                f(o, *args, **kwargs)
                o._callback(f'after_{self.name}')
            except globals()[f'Cancel{self.name.title()}Exception']: pass
            finally: o._callback(f'cleanup_{self.name}')
        return _f

class Learner():
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):
        self.model = model
        self.dls = dls
        self.loss_func=loss_func
        self.lr=lr
        self.cbs = fc.L(cbs)
        self.opt_func = opt_func

    def fit(self, n_epochs=1, train=True, valid=True, lr=None, cbs=None):
        cbs = fc.L(cbs)
        for cb in cbs: self.cbs.append(cb) # add extra cbs
        try:
            self.n_epochs = n_epochs            
            self.epochs = range(n_epochs)
            self.model.train(train) # Set model training before any callbacks are called
            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)
            self._fit(train, valid)
        finally:
            for cb in cbs: self.cbs.remove(cb) # cleanup extra cbs

    @with_cbs('fit')
    def _fit(self, train, valid):
        for self.epoch in self.epochs:
            if train: 
                self.one_epoch(True)
            if valid:
                with torch.no_grad():
                    self.one_epoch(False)
   
    def one_epoch(self, train):
        self.model.train(train) 
        self.dl = self.dls.train if train else self.dls.valid  
        self.dl_len = len(self.dl)
        self._one_epoch(train)

    @with_cbs('epoch')
    def _one_epoch(self, train):
        for self.iter, self.batch in enumerate(self.dl):
            self.one_batch()
            
    @with_cbs('batch')    
    def one_batch(self):
        self.predict()
        self.get_loss()
        if self.model.training:
            self.backward()
            self.step()
            self.zero_grad()  

    # these are defined here to allow subclassing to customize behavior
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): 
        self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()

    def _callback(self, method_name): run_cbs(self.cbs, method_name, self)
    
    @property
    def training(self):
        return self.model.training

# %% ../nbs/09_learner.ipynb 39
class ProgressCB(Callback):
    order = MetricsCB.order + 1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        self.first = True
        learn.epochs = self.mbar = master_bar(learn.epochs)
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []
        
    def _format_metric(self, m):
        if isinstance(m, float): return f'{m:.3f}'
        return m
        
    def _log(self, recorder):
        d = recorder.get_latest()
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first=False
        self.mbar.write(list(self._format_metric(x) for x in d.values()), table=True)
        
    def before_epoch(self, learn): 
        learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
        
    def after_batch(self, learn):
        learn.dl.comment = f"{learn.loss:.3f}"
        if self.plot and hasattr(learn, 'metrics') and learn.model.training:
            self.losses.append(learn.loss.item())
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])

# %% ../nbs/09_learner.ipynb 41
from torch.utils.tensorboard import SummaryWriter

class TensorboardCB(Callback):
    order = MetricsCB.order + 1    
    def __init__(self, name=None): self.writer = SummaryWriter(comment=f'_{name}')

    def after_batch(self, learner: Learner):        
        # Log loss
        train = 'train' if learn.model.training else 'valid'
        idx = learn.dl_len*learn.epoch + learn.iter
        self.writer.add_scalar(f'loss/{train}', learn.loss.item(), idx)     
        self.writer.flush()

    def after_epoch(self, learn: Learner):
        if hasattr(learn, 'recorder'):
            # Log all other metrics after each epoch
            d = learn.recorder[-1]
            for k, v in d.items():
                if k == 'loss': continue
                self.writer.add_scalar(f'{k}/{d.train}', v, d.epoch)    
            self.writer.flush()
            
    def after_fit(self, learner: Learner): self.writer.close()

# %% ../nbs/09_learner.ipynb 42
import wandb
from pathlib import Path


class Config(dict):
    def __init__(self, ds, arch, **kwargs):
        opts = { 'epochs': 5, 'lr': 1e-4, 'model_path': 'models' }
        opts.update(kwargs)
        super().__init__(ds=ds, arch=arch, **opts)

    def __getattr__(self, key):
        if key in self:
            return self[key]
        return super().__getattr__(self, key)

class WandBCB(Callback):
    order = MetricsCB.order + 1
    def __init__(self, config: Config): 
        self.config = config
        
    def before_fit(self, learn: Learner): wandb.init(project=self.config.project, config=self.config)        
    def after_fit(self, learn: Learner): 
        if self.config.save:
            model_path = Path(self.config.model_path + '/' + wandb.run.project)
            model_path.mkdir(exist_ok=True, parents=True)
            torch.save(learn.model, model_path/ f"{wandb.run.name}.pkl")
            print(f"Saved model to {model_path/wandb.run.name}")
            wandb.finish()        

    def after_batch(self, learn: Learner):
        # Log loss
        step = learn.dl_len*learn.epoch + learn.iter
        train = 'train' if learn.model.training else 'valid'
        wandb.log({f'{train}/loss': learn.loss.item()}, step)
    
    def after_epoch(self, learn: Learner):
        if not hasattr(learn, 'recorder'): return # skip if we don't have metrics to log
        # Log all metrics
        latest = learn.recorder[-1]
        step = learn.dl_len*learn.epoch + learn.iter        
        wandb.log({f'{latest.train}/{k}': v for k, v in latest.items()}, step)    


# %% ../nbs/09_learner.ipynb 46
class MomentumLearner(Learner):
    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):
        self.mom = mom
        super().__init__(model, dls, loss_func, lr, cbs, opt_func)
    
    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters():
                # Instead of zeroing out the gradients, we just keep a residue
                p.grad *= self.mom 

# %% ../nbs/09_learner.ipynb 52
from torch.optim.lr_scheduler import ExponentialLR

# %% ../nbs/09_learner.ipynb 53
class LRFinderCB(Callback):
    def __init__(self, gamma=1.3, max_mult=3, verbose=False): fc.store_attr()
    
    def before_fit(self, learn):
        self.scheduler = ExponentialLR(learn.opt, self.gamma)
        self.lrs, self.losses = [], []
        self.min = math.inf
        
    def after_batch(self, learn):        
        if not learn.model.training: raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if self.verbose:
            print(f"lr={learn.opt.param_groups[0]['lr']:.2f}\tloss={loss:.2f}\tmin_loss={self.min:.2f}")

        if loss < self.min: self.min = loss
        if math.isnan(loss) or loss > self.max_mult*self.min: 
            raise CancelFitException()
        self.scheduler.step()
        
    def cleanup_fit(self, learn):
        plt.plot(self.lrs, self.losses)
        plt.xscale('log')

# %% ../nbs/09_learner.ipynb 54
@fc.patch
def lr_find(self: Learner, start_lr=1e-5, gamma=1.3, max_mult=3, max_epochs=10, verbose=False):
    self.fit(n_epochs=max_epochs, lr=start_lr, cbs=[DeviceCB(), LRFinderCB(gamma=gamma, max_mult=max_mult, verbose=verbose)])

# %% ../nbs/09_learner.ipynb 56
class SingleBatchCB(Callback):
    order = 1
    def __init__(self, verbose=False): self.verbose = verbose
    def after_batch(self, learn: Learner):
        if self.verbose: print(f"Stopping at epoch {learn.epoch_idx}")
        raise CancelFitException
